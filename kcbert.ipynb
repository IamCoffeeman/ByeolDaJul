{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzn2fmGumqrp6T00ax/bqQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyeokkim/ByeolDaJul/blob/main/kcbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "EjSpaijkvpqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "c06hB7TxFtX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUmrKmjmvXRJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리들을 임포트합니다.\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터를 불러오는 함수를 정의합니다.\n",
        "def load_and_parse_data(filepath):\n",
        "    data = {\n",
        "        \"label\": [],\n",
        "        \"intent_pair1\": [],\n",
        "        \"intent_pair2\": []\n",
        "    }\n",
        "\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            data[\"label\"].append(int(parts[0]))\n",
        "            data[\"intent_pair1\"].append(parts[1])\n",
        "            data[\"intent_pair2\"].append(parts[2])\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# 위에서 정의한 함수를 사용하여 데이터를 불러옵니다.\n",
        "df = load_and_parse_data('/content/sae4k_v2.txt')\n",
        "\n",
        "# KcBERT의 토크나이저와 모델을 불러옵니다.\n",
        "model_name = \"beomi/kcbert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=df['label'].nunique())\n",
        "\n",
        "# 데이터셋 클래스를 정의합니다. 이 클래스는 토큰화와 인코딩을 수행합니다.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': label}\n",
        "\n",
        "# 데이터셋과 DataLoader를 준비합니다.\n",
        "texts = df['intent_pair1'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "max_length = 128\n",
        "dataset = CustomDataset(texts, labels, tokenizer, max_length)\n",
        "\n",
        "# 데이터셋을 훈련용과 검증용으로 분리합니다.\n",
        "train_dataset, valid_dataset = train_test_split(dataset, test_size=0.15, random_state=42)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "RnOTt04ik8EH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5937a8-0b6e-4bb5-ae54-5a5bbe447561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델을 훈련하고 검증하는 코드가 들어갑니다.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "learning_rate = 1e-5\n",
        "epochs = 5\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "batch_size = 16\n"
      ],
      "metadata": {
        "id": "Ic9syAadllwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1035ec33-b00c-44b0-9e43-1afa8a7ff673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import compute_accuracy\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # 그래디언트 계산을 비활성화\n",
        "        for data in data_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "ii9-P7ryN9N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# 원하는 데이터의 일부 비율 설정 (예: 1%)\n",
        "fraction = 0.01\n",
        "num_samples = int(fraction * len(valid_dataloader.dataset))\n",
        "\n",
        "# 데이터셋에서 일부만 샘플링\n",
        "partial_valid_dataset, _ = random_split(valid_dataloader.dataset, [num_samples, len(valid_dataloader.dataset) - num_samples])\n",
        "\n",
        "# 새로운 DataLoader 생성\n",
        "partial_valid_dataloader = DataLoader(partial_valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "accuracy = compute_accuracy(model, partial_valid_dataloader, device)\n",
        "print(f\"Partial Validation Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "yTNC1UZkl0W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# 데이터 불러오기 및 파싱 함수 정의\n",
        "def load_and_parse_data(filepath):\n",
        "    data = {\"label\": [], \"intent_pair1\": [], \"intent_pair2\": []}\n",
        "\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            data[\"label\"].append(int(parts[0]))\n",
        "            data[\"intent_pair1\"].append(parts[1])\n",
        "            data[\"intent_pair2\"].append(parts[2])\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# 데이터 불러오기\n",
        "filepath = '/content/sae4k_v2.txt'\n",
        "df = load_and_parse_data(filepath)\n",
        "\n",
        "# 데이터 샘플링 (원본의 5% 사용)\n",
        "sample_frac = 0.05\n",
        "df = df.sample(frac=sample_frac, random_state=42)\n",
        "\n",
        "# KcBERT 토크나이저 및 모델 초기화\n",
        "tokenizer = BertTokenizer.from_pretrained('beomi/kcbert-base')\n",
        "model = BertForSequenceClassification.from_pretrained('beomi/kcbert-base', num_labels=df['label'].nunique())\n",
        "\n",
        "# CUDA 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 데이터 토큰화 및 DataLoader 생성\n",
        "def tokenize_data(df, tokenizer):\n",
        "    sentences = df['intent_pair1'].values\n",
        "    labels = df['label'].values\n",
        "\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=64,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "input_ids, attention_masks, labels = tokenize_data(df, tokenizer)\n",
        "\n",
        "# 데이터셋 분할\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.1)\n",
        "\n",
        "# DataLoader 설정\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "# 모델 학습 설정\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 1\n",
        "\n",
        "# 모델 학습\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    for step, batch in enumerate(progress):\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {avg_train_loss:.2f}\")\n"
      ],
      "metadata": {
        "id": "j8R3foDAO6yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_label(label):\n",
        "    labels = [\n",
        "        \"0 (예/아니오)\",\n",
        "        \"1 (대체적인)\",\n",
        "        \"2 (wh- 질문)\",\n",
        "        \"3 (금지)\",\n",
        "        \"4 (요구 사항)\",\n",
        "        \"5 (강력한 요구 사항)\"\n",
        "    ]\n",
        "    return labels[label]\n",
        "\n",
        "# 테스트하고자 하는 텍스트 데이터\n",
        "input_data = [\n",
        "    \"나 물좀 가져다줘\"\n",
        "]\n",
        "\n",
        "input_encodings = tokenizer(input_data, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 모델에 입력 데이터 전달\n",
        "with torch.no_grad():\n",
        "    output = model(**input_encodings)\n",
        "\n",
        "# 예측 결과 확인\n",
        "logits = output.logits\n",
        "predicted_labels = logits.argmax(dim=1)\n",
        "\n",
        "# 예측 결과 출력\n",
        "for i, input_text in enumerate(input_data):\n",
        "    predicted_label = predicted_labels[i].item()\n",
        "    print(f\"Input: {input_text} - Predicted Label: {valid_label(predicted_label)}\")\n"
      ],
      "metadata": {
        "id": "91tdl6HKl8KQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79e19a4-f823-4337-8a30-07e76833cac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 나 물좀 가져다줘 - Predicted Label: 5 (강력한 요구 사항)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "model_save_path = \"kc_bert_classifier.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "# 모델 로드\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=df['label'].nunique())\n",
        "loaded_model.load_state_dict(torch.load(model_save_path))\n",
        "loaded_model.eval()"
      ],
      "metadata": {
        "id": "8x5brqQ9opW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kcbert"
      ],
      "metadata": {
        "id": "OIhVj6wXo8pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soynlp emoji\n",
        "!pip install torch torchvision\n",
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "BhLDnGA_ueVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wFznhQ5J081",
        "outputId": "4af89dc0-9b69-4966-e21c-84b8780884aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣]+')\n",
        "url_pattern = re.compile(\n",
        "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
        "\n",
        "def clean(x):\n",
        "    x = pattern.sub(' ', x)\n",
        "    x = emoji.replace_emoji(x, replace='') #emoji 삭제\n",
        "    x = url_pattern.sub('', x)\n",
        "    x = x.strip()\n",
        "    x = repeat_normalize(x, num_repeats=2)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "dkmPdJ_By_e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "# 데이터 불러오기 함수\n",
        "def load_and_parse_data(filepath):\n",
        "    data = {\n",
        "        \"label\": [],\n",
        "        \"intent_pair1\": [],\n",
        "        \"intent_pair2\": []\n",
        "    }\n",
        "\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            data[\"label\"].append(int(parts[0]))\n",
        "            data[\"intent_pair1\"].append(parts[1])\n",
        "            data[\"intent_pair2\"].append(parts[2])\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# 데이터 불러오기\n",
        "df = load_and_parse_data('/content/sae4k_v2.txt')\n",
        "\n",
        "# 원래 데이터셋의 5%만 사용\n",
        "sample_frac = 0.05\n",
        "df = df.sample(frac=sample_frac, random_state=42)\n",
        "\n",
        "# KcBERT 토크나이저 및 모델 불러오기\n",
        "tokenizer = BertTokenizer.from_pretrained('beomi/kcbert-base')\n",
        "model = BertForSequenceClassification.from_pretrained(\"beomi/kcbert-base\", num_labels=df['label'].nunique())\n",
        "\n",
        "# CUDA 설정 (GPU 사용)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 데이터 토큰화\n",
        "# 2. 데이터 토큰화 및 DataLoader 생성\n",
        "sentences = df['intent_pair1'].values\n",
        "labels = df['label'].values\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 64,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                        truncation=True\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.1)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "# 모델 학습 설정 및 학습\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    progress = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    for step, batch in enumerate(progress):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {avg_train_loss:.2f}\")\n"
      ],
      "metadata": {
        "id": "wpR3HPTEzJK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "평가"
      ],
      "metadata": {
        "id": "j3619hg03hDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten() #flat_accuracy는 예측된 라벨과 실제 라벨을 비교하여 정확도를 계산하는 함수\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ],
      "metadata": {
        "id": "TXU3nszGDBgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# 평가 모드\n",
        "model.eval()\n",
        "\n",
        "# 변수 초기화\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "# 검증 데이터셋에 대해\n",
        "for batch in tqdm(validation_dataloader, desc=\"Evaluating\"):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # 그래디언트 계산 X\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        logits = outputs[0]\n",
        "\n",
        "    # 정확도 계산\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n"
      ],
      "metadata": {
        "id": "CqL6UO013H-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 모델의 가중치 얻기\n",
        "weights = model.state_dict()\n",
        "num_weights = len(weights)\n",
        "\n",
        "# 2. 검증 데이터셋에 대한 정확도 계산\n",
        "\n",
        "# 평가 모드\n",
        "model.eval()\n",
        "\n",
        "# 변수 초기화\n",
        "eval_accuracy = 0\n",
        "nb_eval_steps = 0\n",
        "\n",
        "# 검증 데이터셋에 대해\n",
        "for batch in validation_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # 그래디언트 계산 X\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        logits = outputs[0]\n",
        "\n",
        "    # 정확도 계산\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "accuracy = eval_accuracy/nb_eval_steps\n",
        "\n",
        "print(f\"Number of weights (parameter groups) in the model: {num_weights}\")\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "kKjvr2EVd9P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델의 구조분석,크기 평가\n"
      ],
      "metadata": {
        "id": "IV3rgOuUmxHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.state_dict() # 모델 파라미터\n",
        "\n",
        "total_params = 0\n",
        "# 모델의 각 파라미터에 대해 반복\n",
        "for key, value in weights.items():\n",
        "    num_params = torch.numel(value)\n",
        "    total_params += num_params\n",
        "    print(f\"{key} contains {num_params} parameters\")\n",
        "#총 파라미터 출력\n",
        "print(f\"Total number of parameters in the model: {total_params}\")\n"
      ],
      "metadata": {
        "id": "-w3TL4F8fsl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가 모드\n",
        "model.eval()\n",
        "\n",
        "# 라벨 5의 빈도를 저장할 변수\n",
        "label_5_count = 0\n",
        "\n",
        "# 검증 데이터셋에 대한 예측 수행\n",
        "for batch in validation_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # 그래디언트 계산 X\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        logits = outputs[0]\n",
        "\n",
        "    # 예측된 라벨\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "    # 라벨 5의 빈도 계산\n",
        "    label_5_count += np.sum(predictions == 5)\n",
        "\n",
        "print(f\"Label 5 appears {label_5_count} times in the predictions.\")\n"
      ],
      "metadata": {
        "id": "_8qer89XZUmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "휴먼평가"
      ],
      "metadata": {
        "id": "5J_GCDsk-_32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def flat_accuracy(preds, labels): #flat_accuracy는 예측된 라벨과 실제 라벨을 비교하여 정확도를 계산하는 함수\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ],
      "metadata": {
        "id": "odwkWbJN3Kbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, model, tokenizer):\n",
        "    # 모델을 평가 모드로 설정합니다. 이는 'dropout'과 같은 학습 중 특정 기능을 비활성화합니다.\n",
        "    model.eval()\n",
        "\n",
        "    # 주어진 텍스트를 모델이 이해할 수 있는 형태로 인코딩합니다.\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text,\n",
        "        None,\n",
        "        add_special_tokens=True, # 특수 토큰(예: [CLS], [SEP]) 추가\n",
        "        max_length=64,\n",
        "        pad_to_max_length=True, #  패딩\n",
        "        return_token_type_ids=True, # 토큰 타입 ID 반환 설정\n",
        "        truncation=True # 최대 길이 초과 시 잘라냄\n",
        "    )\n",
        "\n",
        "    # 인코딩된 텍스트를 텐서로 변환하고, 모델이 있는 장치로 이동시킵니다.\n",
        "    input_ids = torch.tensor([inputs[\"input_ids\"]]).to(device)\n",
        "    attention_mask = torch.tensor([inputs[\"attention_mask\"]]).to(device)\n",
        "\n",
        "    # 모델의 예측을 계산합니다. 이때, 기울기 계산을 방지하기 위해 torch.no_grad()를 사용합니다.\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, token_type_ids=None, attention_mask=attention_mask)[0]\n",
        "\n",
        "    # 로짓을 NumPy 배열로 변환합니다.\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    # 가장 높은 확률을 가진 클래스의 인덱스를 예측값으로 선택합니다.\n",
        "    prediction = np.argmax(logits, axis=1)\n",
        "\n",
        "    # 예측된 클래스의 인덱스를 반환합니다.\n",
        "    return prediction[0]\n"
      ],
      "metadata": {
        "id": "V4WXBwyQ3Ltk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" 관악산 등반할 때 일반 운동화가 아닌 등산화를 신어야해\"\n",
        "predicted_label = predict(text, model, tokenizer)\n",
        "print(f\"Predicted label for text '{text}': {predicted_label}\")\n"
      ],
      "metadata": {
        "id": "S7atHKKX3NkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터에서 5번 라벨로 분류된 텍스트 샘플 추출\n",
        "label_5_texts = df[df['label'] == 5]['intent_pair1'].tolist()\n",
        "\n",
        "# 5번 라벨로 분류된 텍스트로 예측 수행\n",
        "for text in label_5_texts:\n",
        "    predicted_label = predict(text, model, tokenizer)\n",
        "    print(f\"Text: {text} -> Predicted Label: {predicted_label}\")\n"
      ],
      "metadata": {
        "id": "fI1wET4cTgRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#시각화"
      ],
      "metadata": {
        "id": "KTKRUnzZln_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz\n"
      ],
      "metadata": {
        "id": "G4IfL7LGh6yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchviz import make_dot\n",
        "\n",
        "# 'tokenizer'가 정의되어 있고, `model_max_length` 메서드를 가지고 있다고 가정합니다.\n",
        "# 인덱스의 반복 패턴으로 채워진 텐서를 생성하고, LongTensor 타입이 되도록 합니다.\n",
        "dummy_input = torch.randint(0, tokenizer.vocab_size, (1, tokenizer.model_max_length)).to(device).long()\n",
        "\n",
        "# 순전파를 수행합니다.\n",
        "outputs = model(dummy_input)\n",
        "logits = outputs[0]\n",
        "\n",
        "# 시각화를 위한 코드입니다.\n",
        "dot = make_dot(logits, params=dict(model.named_parameters()))\n",
        "dot.format = 'png'\n",
        "dot.render(\"model_graph\")\n"
      ],
      "metadata": {
        "id": "YJs5QHvji2dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 가중치 저장\n",
        "model_save_path = \"/content/kcbert_finetuned_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n"
      ],
      "metadata": {
        "id": "_BuQQiF7afYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 인스턴스 생성\n",
        "model = BertForSequenceClassification.from_pretrained(\"beomi/kcbert-base\", num_labels=df['label'].nunique())\n",
        "model.to(device)\n",
        "\n",
        "# 저장된 가중치 로드\n",
        "model.load_state_dict(torch.load(model_save_path))\n"
      ],
      "metadata": {
        "id": "FoJzHeNYSsmb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}