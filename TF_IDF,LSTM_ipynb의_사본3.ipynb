{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunghyeokkim/ByeolDaJul/blob/main/TF_IDF%2CLSTM_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B83.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQQi2KLBWn_r",
        "outputId": "326773b8-d621-4541-c14d-d24087431528"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m244.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLmC3QRIBn3X",
        "outputId": "2491a807-a7c3-4c82-c8d9-5f68d051aff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=/content/sae4k_v2.txt\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n"
          ]
        }
      ],
      "source": [
        "!gdown /content/sae4k_v2.txt #파일 다운"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "5Z23nNSJRcJG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "ATecT88TNNR6",
        "outputId": "d33609f8-9672-4cd7-9ee8-89cef645243a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sae4k_v2.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-67109f0799c2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 파일 내용 열고 읽기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sae4k_v2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 파일 보기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sae4k_v2.txt'"
          ]
        }
      ],
      "source": [
        "# 파일 내용 열고 읽기\n",
        "with open(\"/content/sae4k_v2.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "# 파일 보기\n",
        "content_preview = content[:]\n",
        "content_preview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIJvYNc6OX8U"
      },
      "outputs": [],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWj2K09KQGaw"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "def extract_important_words(sentence: str, n=3) -> str:\n",
        "    \"\"\"\n",
        "    Okt와 TF-IDF를 사용하여 문장에서 n개의 가장 중요한 단어를 추출합니다.\n",
        "    \"\"\"\n",
        "    # Okt를 사용하여 문장 토큰화\n",
        "    words = okt.nouns(sentence)  # 간단하게 하기 위해 명사만 추출합니다.\n",
        "\n",
        "    # 단어가 n개 이하이면 모든 단어를 반환\n",
        "    if len(words) <= n:\n",
        "        return ' '.join(words)\n",
        "\n",
        "    # 그렇지 않으면 각 단어의 TF-IDF 점수를 계산하고 상위 n개의 단어를 선택\n",
        "    vectorizer = TfidfVectorizer().fit(words)\n",
        "    word_tfidf_scores = vectorizer.transform(words).sum(axis=0).tolist()[0]\n",
        "\n",
        "    sorted_words = [word for _, word in sorted(zip(word_tfidf_scores, words), reverse=True)]\n",
        "\n",
        "    return ' '.join(sorted_words[:n])\n",
        "\n",
        "# 함수 테스트\n",
        "sample_sentence = \"지금 너희 집으로 가고있는데 몇시쯤 도착해?\"\n",
        "important_words = extract_important_words(sample_sentence, 3)\n",
        "print(important_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNWZDQrS1qAH"
      },
      "source": [
        "# Lstm모델을 이용한 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfcarh8st0VG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. 데이터 로딩\n",
        "def load_and_parse_data(filepath):\n",
        "    data = {\n",
        "        \"label\": [],\n",
        "        \"intent_pair1\": [],\n",
        "        \"intent_pair2\": []\n",
        "    }\n",
        "\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            data[\"label\"].append(int(parts[0]))\n",
        "            data[\"intent_pair1\"].append(parts[1])\n",
        "            data[\"intent_pair2\"].append(parts[2])\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "df = load_and_parse_data(\"/content/sae4k_v2.txt\")\n",
        "\n",
        "# 2. 데이터 전처리\n",
        "MAX_WORDS = 10000 # 고려할 최대 단어수 = 최빈값 10000개만 고려하기\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>') #케라스 제공 텍스트 토큰화, ov_token='<OOV>' =  본적없는 단어 나올시 대체(일반화)\n",
        "tokenizer.fit_on_texts(df['intent_pair1']) #텍스트 토큰화 진행\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df['intent_pair1']) # 텍스트 데이터를 숫자 시퀀스로 변환\n",
        "X = pad_sequences(sequences)\n",
        "y = to_categorical(df['label'])\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42) #분할 비율 (70% 학습, 15% 검증, 15% 테스트)더 많은 데이터 80 10 10도 가능\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# 3. 모델 구성 및 학습\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(MAX_WORDS, 128, input_length=X.shape[1]), #Embedding 층: 주어진 단어의 정수 인덱스를 벡터 형태로 변환\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True), #LSTM 층 (128 뉴런) True 옵션을 사용해서 다음층으로 전달\n",
        "    tf.keras.layers.LSTM(64), #추가처리 64층\n",
        "    tf.keras.layers.Dense(64, activation='relu'), #Dense 층 (64 뉴런): 히든 레이어로, ReLU 활성화 함수를 사용\n",
        "    tf.keras.layers.Dropout(0.5), # Dropout 층: 과적합을 방지하기 위해 사용됩\n",
        "    tf.keras.layers.Dense(len(df['label'].unique()), activation='softmax') # Dense 층: 최종 출력 층으로, 다중 분류를 위한 softmax 활성화 함수사용\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of weights in the model\n",
        "num_weights = sum([tf.size(w).numpy() for w in model.weights])\n",
        "\n",
        "# Get the training accuracy from the history object\n",
        "final_training_accuracy = history.history['accuracy'][-1]\n",
        "\n",
        "num_weights, final_training_accuracy\n"
      ],
      "metadata": {
        "id": "uJ5Lw3qxxO4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg52wl1r0COO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfauqyeYzuBi"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test) #테스트 데이터로 모델 성능평가\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwSDmUjS0tZj"
      },
      "outputs": [],
      "source": [
        "# 새로운 데이터 넣어서 모델 평가해보기\n",
        "# 레이블의 인덱스와 실제 레이블 값의 매핑 정보를 포함하는 한글 리스트를 생성\n",
        "LABELS = [\"예/아니오\", \"대체\", \"wh- 질문\", \"금지\", \"요구사항\", \"강한 요구사항\"]\n",
        "\n",
        "def predict_intent(sentence):\n",
        "    # 토큰화 및 패딩\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=X.shape[1])\n",
        "\n",
        "    # 예측\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    predicted_label = LABELS[prediction.argmax()]\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# 예측 테스트\n",
        "sentence = \"필수적으로 진행해 주세요\"\n",
        "print(f\"Predicted Intent: {predict_intent(sentence)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqzGVTX11d1f"
      },
      "outputs": [],
      "source": [
        "# 강한 요구사항을 출력 어려움"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TvzQ688rBq7j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0T7HvYjKELd4LQhKcxj1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}